{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKICvl5t4ZK0",
        "outputId": "cfe32f95-b09c-4f88-8240-b798fbe01a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total transactions in dataset: 1000\n",
            "Split date (65th percentile): 2024-08-15 15:59:00\n",
            "Train set size: 650\n",
            "Test set size: 350\n",
            "Average Category Overlap Hit Rate on Test Data: 0.684\n",
            "\n",
            "=== Top 3 Lookalike Users for the First 20 Customers ===\n",
            "\n",
            "CustomerID: C0001\n",
            "  Lookalike 1: C0156 with similarity score 0.7596\n",
            "  Lookalike 2: C0129 with similarity score 0.6839\n",
            "  Lookalike 3: C0020 with similarity score 0.5633\n",
            "\n",
            "CustomerID: C0002\n",
            "  Lookalike 1: C0030 with similarity score 0.5552\n",
            "  Lookalike 2: C0007 with similarity score 0.5518\n",
            "  Lookalike 3: C0138 with similarity score 0.5490\n",
            "\n",
            "CustomerID: C0003\n",
            "  Lookalike 1: C0134 with similarity score 0.8185\n",
            "  Lookalike 2: C0181 with similarity score 0.6975\n",
            "  Lookalike 3: C0144 with similarity score 0.5021\n",
            "\n",
            "CustomerID: C0004\n",
            "  Lookalike 1: C0025 with similarity score 0.5472\n",
            "  Lookalike 2: C0124 with similarity score 0.5438\n",
            "  Lookalike 3: C0065 with similarity score 0.5233\n",
            "\n",
            "CustomerID: C0005\n",
            "  Lookalike 1: C0096 with similarity score 0.5025\n",
            "  Lookalike 2: C0072 with similarity score 0.4899\n",
            "  Lookalike 3: C0168 with similarity score 0.4367\n",
            "\n",
            "CustomerID: C0006\n",
            "  Lookalike 1: C0013 with similarity score 0.7052\n",
            "  Lookalike 2: C0040 with similarity score 0.7006\n",
            "  Lookalike 3: C0046 with similarity score 0.6415\n",
            "\n",
            "CustomerID: C0007\n",
            "  Lookalike 1: C0118 with similarity score 0.7078\n",
            "  Lookalike 2: C0002 with similarity score 0.5518\n",
            "  Lookalike 3: C0036 with similarity score 0.5311\n",
            "\n",
            "CustomerID: C0008\n",
            "  Lookalike 1: C0091 with similarity score 0.6890\n",
            "  Lookalike 2: C0143 with similarity score 0.6777\n",
            "  Lookalike 3: C0162 with similarity score 0.6629\n",
            "\n",
            "CustomerID: C0009\n",
            "  Lookalike 1: C0042 with similarity score 0.7917\n",
            "  Lookalike 2: C0083 with similarity score 0.7001\n",
            "  Lookalike 3: C0189 with similarity score 0.5842\n",
            "\n",
            "CustomerID: C0010\n",
            "  Lookalike 1: C0189 with similarity score 0.8502\n",
            "  Lookalike 2: C0033 with similarity score 0.8495\n",
            "  Lookalike 3: C0094 with similarity score 0.6593\n",
            "\n",
            "CustomerID: C0011\n",
            "  Lookalike 1: C0109 with similarity score 0.6608\n",
            "  Lookalike 2: C0113 with similarity score 0.5390\n",
            "  Lookalike 3: C0107 with similarity score 0.5304\n",
            "\n",
            "CustomerID: C0012\n",
            "  Lookalike 1: C0090 with similarity score 0.6065\n",
            "  Lookalike 2: C0168 with similarity score 0.5747\n",
            "  Lookalike 3: C0065 with similarity score 0.5465\n",
            "\n",
            "CustomerID: C0013\n",
            "  Lookalike 1: C0046 with similarity score 0.7850\n",
            "  Lookalike 2: C0040 with similarity score 0.7745\n",
            "  Lookalike 3: C0006 with similarity score 0.7052\n",
            "\n",
            "CustomerID: C0014 has no lookalikes.\n",
            "CustomerID: C0015\n",
            "  Lookalike 1: C0126 with similarity score 0.7015\n",
            "  Lookalike 2: C0050 with similarity score 0.6198\n",
            "  Lookalike 3: C0097 with similarity score 0.5563\n",
            "\n",
            "CustomerID: C0016\n",
            "  Lookalike 1: C0196 with similarity score 0.6244\n",
            "  Lookalike 2: C0192 with similarity score 0.6149\n",
            "  Lookalike 3: C0187 with similarity score 0.5945\n",
            "\n",
            "CustomerID: C0017\n",
            "  Lookalike 1: C0055 with similarity score 0.7025\n",
            "  Lookalike 2: C0183 with similarity score 0.5520\n",
            "  Lookalike 3: C0028 with similarity score 0.5417\n",
            "\n",
            "CustomerID: C0018 has no lookalikes.\n",
            "CustomerID: C0019\n",
            "  Lookalike 1: C0196 with similarity score 0.6429\n",
            "  Lookalike 2: C0064 with similarity score 0.6320\n",
            "  Lookalike 3: C0026 with similarity score 0.6194\n",
            "\n",
            "Lookalike mappings for the first 20 customers have been saved to 'Lookalike.csv'\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Steps:\n",
        "    1) Load and merge data (Customers + Transactions)\n",
        "    2) Feature engineering (total spend, average order, tenure, region)\n",
        "    3) Outlier capping + optional log(1+x) transform\n",
        "    4) Try multiple clustering algorithms with different parameters:\n",
        "        - K-Means\n",
        "        - Agglomerative Clustering\n",
        "        - Gaussian Mixture\n",
        "        - DBSCAN\n",
        "    5) Record metrics (DB, silhouette, etc.) and pick the best DB.\n",
        "    6) Output final labels and top configurations.\n",
        "    7) Print top 3 lookalikes for the first 20 customers and save to Lookalike.csv\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv('Merged_DataFrame.csv', dayfirst=True)\n",
        "\n",
        "# Ensure proper date formatting and drop rows with missing essential information\n",
        "data['TransactionDate'] = pd.to_datetime(data['TransactionDate'], errors='coerce', dayfirst=True)\n",
        "data.dropna(subset=['CustomerID', 'ProductID', 'TransactionDate'], inplace=True)\n",
        "\n",
        "# Sort the dataset by transaction date for a time-based split\n",
        "data_sorted = data.sort_values(by='TransactionDate').reset_index(drop=True)\n",
        "\n",
        "n = len(data_sorted)\n",
        "print(f\"Total transactions in dataset: {n}\")\n",
        "\n",
        "# Determine the split point for a 65/35 time-based split\n",
        "train_ratio = 0.65\n",
        "split_index = int(n * train_ratio)\n",
        "split_date = data_sorted.loc[split_index, 'TransactionDate']\n",
        "\n",
        "print(f\"Split date (65th percentile): {split_date}\")\n",
        "\n",
        "# Create training and test datasets based on the split date\n",
        "train_data = data[data['TransactionDate'] < split_date]\n",
        "test_data = data[data['TransactionDate'] >= split_date]\n",
        "\n",
        "print(\"Train set size:\", len(train_data))\n",
        "print(\"Test set size:\", len(test_data))\n",
        "\n",
        "# Fill missing values in key columns of the training data\n",
        "train_data.loc[:, 'Region'] = train_data['Region'].fillna('Unknown')\n",
        "train_data.loc[:, 'Category'] = train_data['Category'].fillna('Unknown')\n",
        "train_data.loc[:, 'ProductName'] = train_data['ProductName'].fillna('Unknown')\n",
        "train_data.loc[:, 'TotalValue'] = train_data['TotalValue'].fillna(0)\n",
        "train_data.loc[:, 'Quantity'] = train_data['Quantity'].fillna(0)\n",
        "\n",
        "# Aggregate customer-level data for analysis and feature creation\n",
        "user_agg = train_data.groupby('CustomerID').agg(\n",
        "    TotalExpenses=('TotalValue', 'sum'),\n",
        "    TransactionCount=('TransactionDate', 'count'),\n",
        "    LastPurchaseDate=('TransactionDate', 'max'),\n",
        "    Region=('Region', lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')  # Handle empty mode\n",
        ").reset_index()\n",
        "\n",
        "# Compute additional metrics like average order value (AOV) and recency\n",
        "user_agg['AOV'] = user_agg['TotalExpenses'] / user_agg['TransactionCount']\n",
        "reference_date_train = train_data['TransactionDate'].max()\n",
        "user_agg['Recency'] = (reference_date_train - user_agg['LastPurchaseDate']).dt.days\n",
        "\n",
        "# Measure diversity in categories purchased\n",
        "user_agg['CategoryDiversity'] = train_data.groupby('CustomerID')['Category'].nunique().values\n",
        "\n",
        "# Analyze how each customer interacts with product categories\n",
        "user_cat_matrix = train_data.pivot_table(\n",
        "    index='CustomerID',\n",
        "    columns='Category',\n",
        "    values='Quantity',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "user_cat_distribution = user_cat_matrix.div(user_cat_matrix.sum(axis=1), axis=0).fillna(0)\n",
        "\n",
        "# Create text-based profiles for customers using product and category information\n",
        "def concat_product_text(df):\n",
        "    return \" \".join(df['ProductName'] + \" \" + df['Category'])\n",
        "\n",
        "user_text_df = (\n",
        "    train_data.groupby('CustomerID')[['ProductName', 'Category']]\n",
        "    .apply(concat_product_text)\n",
        "    .reset_index(name='AllProductsText')\n",
        ")\n",
        "\n",
        "# Use TF-IDF to process text-based profiles and limit features for simplicity\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "tfidf_matrix = tfidf.fit_transform(user_text_df['AllProductsText'])\n",
        "user_text_df = user_text_df.set_index('CustomerID')\n",
        "tfidf_user_ids = user_text_df.index.tolist()\n",
        "tfidf_index_map = {uid: i for i, uid in enumerate(tfidf_user_ids)}\n",
        "\n",
        "# Combine all features (aggregates, category distribution, and text profiles)\n",
        "user_agg = user_agg.set_index('CustomerID')\n",
        "combined_features_df = user_agg.join(user_cat_distribution, how='left').join(user_text_df, how='left')\n",
        "combined_features_df.fillna(0, inplace=True)\n",
        "\n",
        "# Encode categorical data (Region) and standardize numeric features\n",
        "region_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "region_encoded = region_encoder.fit_transform(combined_features_df[['Region']])\n",
        "\n",
        "numeric_cols = ['TotalExpenses', 'AOV', 'Recency', 'CategoryDiversity'] + list(user_cat_distribution.columns)\n",
        "numeric_data = combined_features_df[numeric_cols].values\n",
        "scaler = StandardScaler()\n",
        "numeric_data_scaled = scaler.fit_transform(numeric_data)\n",
        "\n",
        "# Build context vectors for each user by combining features\n",
        "final_user_ids = combined_features_df.index.tolist()\n",
        "context_vectors = []\n",
        "for i, uid in enumerate(final_user_ids):\n",
        "    numeric_region_vec = np.hstack([numeric_data_scaled[i], region_encoded[i]])\n",
        "    text_vec = tfidf_matrix[tfidf_index_map[uid]].toarray().flatten() if uid in tfidf_index_map else np.zeros(tfidf_matrix.shape[1])\n",
        "    combined_vec = np.hstack([numeric_region_vec, text_vec])\n",
        "    context_vectors.append(combined_vec)\n",
        "\n",
        "context_vectors = np.array(context_vectors)\n",
        "context_user_similarity = cosine_similarity(context_vectors)\n",
        "user_index_map = {uid: idx for idx, uid in enumerate(final_user_ids)}\n",
        "\n",
        "# Perform collaborative filtering using SVD on the user-item interaction matrix\n",
        "user_item_matrix = train_data.pivot_table(\n",
        "    index='CustomerID',\n",
        "    columns='ProductID',\n",
        "    values='Quantity',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "svd = TruncatedSVD(n_components=20, random_state=42)\n",
        "user_embeddings_svd = svd.fit_transform(user_item_matrix)\n",
        "cf_user_ids = user_item_matrix.index.tolist()\n",
        "cf_user_index_map = {uid: i for i, uid in enumerate(cf_user_ids)}\n",
        "transaction_sim = cosine_similarity(user_embeddings_svd)\n",
        "\n",
        "# Combine similarities from both content and collaborative filtering approaches\n",
        "def get_combined_user_similarity(user_id, alpha=0.5):\n",
        "    if (user_id not in user_index_map) or (user_id not in cf_user_index_map):\n",
        "        return None\n",
        "    ctx_idx = user_index_map[user_id]\n",
        "    cf_idx = cf_user_index_map[user_id]\n",
        "    sim_context_vec = context_user_similarity[ctx_idx]\n",
        "    sim_cf_vec = transaction_sim[cf_idx]\n",
        "    return alpha * sim_context_vec + (1 - alpha) * sim_cf_vec\n",
        "\n",
        "def get_top_lookalikes(user_id, top_n=3, alpha=0.5):\n",
        "    combined_sim = get_combined_user_similarity(user_id, alpha=alpha)\n",
        "    if combined_sim is None:\n",
        "        return []\n",
        "    pairs = [(uid, combined_sim[i]) for i, uid in enumerate(final_user_ids) if uid != user_id]\n",
        "    return sorted(pairs, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# Evaluate the model by measuring category overlap between users and their lookalikes\n",
        "def evaluate_lookalike_hit_rate_by_category(test_data, lookalike_dict):\n",
        "    user_test_categories = test_data.groupby('CustomerID')['Category'].apply(set).to_dict()\n",
        "    hit_rates = []\n",
        "    for user_id, lookalikes in lookalike_dict.items():\n",
        "        if user_id not in user_test_categories or not user_test_categories[user_id]:\n",
        "            continue\n",
        "        user_cats = user_test_categories[user_id]\n",
        "        union_lk_cats = set()\n",
        "        for lk_id, _ in lookalikes:\n",
        "            if lk_id in user_test_categories:\n",
        "                union_lk_cats |= user_test_categories[lk_id]\n",
        "        overlap = user_cats & union_lk_cats\n",
        "        hit_rate = len(overlap) / len(user_cats)\n",
        "        hit_rates.append(hit_rate)\n",
        "    return np.mean(hit_rates) if hit_rates else 0.0\n",
        "\n",
        "# Generate recommendations and evaluate\n",
        "lookalike_dict = {uid: get_top_lookalikes(uid, top_n=3, alpha=0.5) for uid in test_data['CustomerID'].unique()}\n",
        "avg_hit_rate_cat = evaluate_lookalike_hit_rate_by_category(test_data, lookalike_dict)\n",
        "print(f\"Average Category Overlap Hit Rate on Test Data: {avg_hit_rate_cat:.3f}\")\n",
        "\n",
        "# =============================================================\n",
        "# 6. PRINT TOP 3 LOOKALIKE USERS FOR FIRST 20 CUSTOMERS AND SAVE TO CSV\n",
        "# =============================================================\n",
        "\n",
        "# Define the first 20 CustomerIDs (C0001 - C0020)\n",
        "# Adjust the range if CustomerIDs don't start at C0001 or have different formatting\n",
        "first_20_customer_ids = [f\"C{str(i).zfill(4)}\" for i in range(1, 20)]\n",
        "\n",
        "# Ensure these CustomerIDs exist in the lookalike_dict\n",
        "existing_customers = [cid for cid in first_20_customer_ids if cid in lookalike_dict]\n",
        "\n",
        "print(\"\\n=== Top 3 Lookalike Users for the First 20 Customers ===\\n\")\n",
        "lookalike_output = []\n",
        "\n",
        "for cid in first_20_customer_ids:\n",
        "    lookalikes = lookalike_dict.get(cid, [])\n",
        "    if not lookalikes:\n",
        "        print(f\"CustomerID: {cid} has no lookalikes.\")\n",
        "        lookalike_output.append({\n",
        "            'CustomerID': cid,\n",
        "            'Lookalike1': None,\n",
        "            'Score1': None,\n",
        "            'Lookalike2': None,\n",
        "            'Score2': None,\n",
        "            'Lookalike3': None,\n",
        "            'Score3': None\n",
        "        })\n",
        "        continue\n",
        "    print(f\"CustomerID: {cid}\")\n",
        "    temp_dict = {'CustomerID': cid}\n",
        "    for rank in range(1, 4):\n",
        "        if rank <= len(lookalikes):\n",
        "            lk_id, score = lookalikes[rank - 1]\n",
        "            print(f\"  Lookalike {rank}: {lk_id} with similarity score {score:.4f}\")\n",
        "            temp_dict[f'Lookalike{rank}'] = lk_id\n",
        "            temp_dict[f'Score{rank}'] = round(score, 4)\n",
        "        else:\n",
        "            print(f\"  Lookalike {rank}: None\")\n",
        "            temp_dict[f'Lookalike{rank}'] = None\n",
        "            temp_dict[f'Score{rank}'] = None\n",
        "    lookalike_output.append(temp_dict)\n",
        "    print()  # For better readability\n",
        "\n",
        "# Convert the lookalike_output list of dictionaries to a DataFrame\n",
        "lookalike_df = pd.DataFrame(lookalike_output)\n",
        "\n",
        "# Ensure that all 20 customers are represented in the DataFrame\n",
        "# If some customers were missing in lookalike_dict, they are already handled above\n",
        "\n",
        "# Save to Lookalike.csv\n",
        "lookalike_df.to_csv('Lookalike.csv', index=False)\n",
        "print(\"Lookalike mappings for the first 20 customers have been saved to 'Lookalike.csv'\")\n"
      ]
    }
  ]
}